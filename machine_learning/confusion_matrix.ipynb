{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "#### In this notebook we have:\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Confusion Matrix](#confusion-matrix)\n",
    "3. [Name of Each Combination](#name-of-each-combination)\n",
    "4. [Table 1](#table-1)\n",
    "5. [Scenario](#scenario)\n",
    "   - [True Positive](#true-positive)\n",
    "   - [True Negative](#true-negative)\n",
    "   - [False Positive](#false-positive)\n",
    "   - [False Negative](#false-negative)\n",
    "6. [Type I and Type II Errors](#type-i-and-type-ii-errors)\n",
    "7. [Confusion Matrix](#confusion-matrix-representation)\n",
    "8. [Table 2](#table-2)\n",
    "9. [Confusion Matrix Example](#confusion-matrix-example)\n",
    "10. [Performance Metrics](#performance-metrics)\n",
    "11. [Accuracy](#accuracy)\n",
    "12. [Accuracy Can Be Misleading](#accuracy-can-be-misleading)\n",
    "13. [Recall or Sensitivity or True Positive Rate](#recall-or-sensitivity-or-true-positive-rate)\n",
    "14. [Specificity or True Negative Rate](#specificity-or-true-negative-rate)\n",
    "15. [Precision](#precision)\n",
    "16. [F1 Score](#f1-score)\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "- A **confusion matrix** is a method to explain the results of a classification model.\n",
    "\n",
    "---\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "- In **binary classification**, the outcomes are:\n",
    "  - **True**\n",
    "  - **False**\n",
    "  \n",
    "- Example: A binary classifier scans MRI images and predicts whether a person has **cancer** or **not**.\n",
    "- The predicted outcomes by the classifier and actual outcomes can have **four combinations**:\n",
    "\n",
    "---\n",
    "\n",
    "## Name of Each Combination\n",
    "\n",
    "| Combination       | Description |\n",
    "|-------------------|-------------|\n",
    "| **True Positive**  | Model predicts **cancer** (positive) and the patient actually has **cancer**. |\n",
    "| **False Positive** | Model predicts **cancer** (positive) but the patient does **not** have cancer. |\n",
    "| **False Negative** | Model predicts **no cancer** (negative) but the patient actually has **cancer**. |\n",
    "| **True Negative**  | Model predicts **no cancer** (negative) and the patient does **not** have cancer. |\n",
    "\n",
    "---\n",
    "\n",
    "## Scenario\n",
    "\n",
    "Consider a classification task (like **pregnant or not pregnant**) performed by a machine learning model.\n",
    "\n",
    "### True Positive\n",
    "- A person who is **actually pregnant** (positive) and classified as **pregnant** (positive).\n",
    "- **TRUE POSITIVE (TP)**\n",
    "\n",
    "### True Negative\n",
    "- A person who is **not pregnant** (negative) and classified as **not pregnant** (negative).\n",
    "- **TRUE NEGATIVE (TN)**\n",
    "\n",
    "### False Positive\n",
    "- A person who is **not pregnant** (negative) but classified as **pregnant** (positive).\n",
    "- **FALSE POSITIVE (FP)**\n",
    "\n",
    "### False Negative\n",
    "- A person who is **pregnant** (positive) but classified as **not pregnant** (negative).\n",
    "- **FALSE NEGATIVE (FN)**\n",
    "\n",
    "---\n",
    "\n",
    "## Type I and Type II Errors\n",
    "\n",
    "- **Type I Error** (False Positive):\n",
    "  - Occurs when you reject the **null hypothesis** when it was **true**.\n",
    "\n",
    "- **Type II Error** (False Negative):\n",
    "  - Occurs when you accept the **null hypothesis** when it was **false**.\n",
    "\n",
    "---\n",
    "\n",
    "## Confusion Matrix Representation\n",
    "\n",
    "A **Confusion Matrix** can be represented as:\n",
    "\n",
    "|               | Predicted Positive | Predicted Negative |\n",
    "|---------------|-------------------|-------------------|\n",
    "| **Actual Positive** | True Positive (TP)  | False Negative (FN) |\n",
    "| **Actual Negative** | False Positive (FP) | True Negative (TN)  |\n",
    "\n",
    "- **Total Predictions** = TP + FP + FN + TN\n",
    "\n",
    "---\n",
    "\n",
    "## Confusion Matrix Example\n",
    "\n",
    "Letâ€™s use real numbers to explain the confusion matrix:\n",
    "\n",
    "- **True Positives (TP)** = 57  \n",
    "- **False Positives (FP)** = 14  \n",
    "- **False Negatives (FN)** = 23  \n",
    "- **True Negatives (TN)** = 171  \n",
    "\n",
    "**Total Predictions** = 57 + 14 + 23 + 171 = **265**\n",
    "\n",
    "---\n",
    "\n",
    "## Performance Metrics\n",
    "\n",
    "Confusion matrix allows us to calculate various performance metrics:\n",
    "\n",
    "---\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "- **Accuracy** is the fraction of correct predictions (both **TP** and **TN**) out of total predictions.\n",
    "  \n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + FP + FN + TN}\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\n",
    "\\frac{57 + 171}{265} = 0.86 \\text{ or } 86\\%\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Accuracy Can Be Misleading\n",
    "\n",
    "- Accuracy alone may be misleading, especially with **class imbalance**.\n",
    "\n",
    "Example:\n",
    "- If a classifier predicts **all negatives** (no cancer cases identified), you may still have high accuracy if **negatives** dominate the dataset.\n",
    "- This is why we need more metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## Recall or Sensitivity or True Positive Rate\n",
    "\n",
    "- **Recall** is the fraction of positive cases correctly predicted out of all **actual positive cases**:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\n",
    "\\frac{57}{57 + 23} = 0.71 \\text{ or } 71\\%\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Specificity or True Negative Rate\n",
    "\n",
    "- **Specificity** is the fraction of negative cases correctly predicted out of all **actual negative cases**:\n",
    "\n",
    "$$\n",
    "\\text{Specificity} = \\frac{TN}{TN + FP}\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\n",
    "\\frac{171}{171 + 14} = 0.92 \\text{ or } 92\\%\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Precision\n",
    "\n",
    "- **Precision** is the fraction of correctly predicted positive cases out of all predicted **positive cases**:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\n",
    "\\frac{57}{57 + 14} = 0.80 \\text{ or } 80\\%\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## F1 Score\n",
    "\n",
    "- The **F1 score** balances **Recall** and **Precision** into a single metric:\n",
    "\n",
    "$$\n",
    "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
